<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Interview Assistant</title>
    <script crossorigin src="https://unpkg.com/react@18/umd/react.production.min.js"></script>
    <script crossorigin src="https://unpkg.com/react-dom@18/umd/react-dom.production.min.js"></script>
    <script src="https://unpkg.com/@babel/standalone/babel.min.js"></script>
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        @keyframes pulse {
            0%, 100% { opacity: 1; }
            50% { opacity: 0.5; }
        }
        .animate-pulse { animation: pulse 2s cubic-bezier(0.4, 0, 0.6, 1) infinite; }
        @keyframes spin {
            from { transform: rotate(0deg); }
            to { transform: rotate(360deg); }
        }
        .animate-spin { animation: spin 1s linear infinite; }
    </style>
</head>
<body>
    <div id="root"></div>

    <script type="text/babel">
        const { useState, useEffect, useRef } = React;

        // Lucide icons as SVG components
        const Mic = ({ size = 24, className = "" }) => (
            <svg width={size} height={size} viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round" className={className}>
                <path d="M12 2a3 3 0 0 0-3 3v7a3 3 0 0 0 6 0V5a3 3 0 0 0-3-3Z"/>
                <path d="M19 10v2a7 7 0 0 1-14 0v-2"/>
                <line x1="12" x2="12" y1="19" y2="22"/>
            </svg>
        );

        const MicOff = ({ size = 24, className = "" }) => (
            <svg width={size} height={size} viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round" className={className}>
                <line x1="2" x2="22" y1="2" y2="22"/>
                <path d="M18.89 13.23A7.12 7.12 0 0 0 19 12v-2"/>
                <path d="M5 10v2a7 7 0 0 0 12 5"/>
                <path d="M15 9.34V5a3 3 0 0 0-5.68-1.33"/>
                <path d="M9 9v3a3 3 0 0 0 5.12 2.12"/>
                <line x1="12" x2="12" y1="19" y2="22"/>
            </svg>
        );

        const Volume2 = ({ size = 24, className = "" }) => (
            <svg width={size} height={size} viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round" className={className}>
                <polygon points="11 5 6 9 2 9 2 15 6 15 11 19 11 5"/>
                <path d="M15.54 8.46a5 5 0 0 1 0 7.07"/>
                <path d="M19.07 4.93a10 10 0 0 1 0 14.14"/>
            </svg>
        );

        const Loader2 = ({ size = 24, className = "" }) => (
            <svg width={size} height={size} viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round" className={className}>
                <path d="M21 12a9 9 0 1 1-6.219-8.56"/>
            </svg>
        );

        const AlertCircle = ({ size = 24, className = "" }) => (
            <svg width={size} height={size} viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round" className={className}>
                <circle cx="12" cy="12" r="10"/>
                <line x1="12" x2="12" y1="8" y2="12"/>
                <line x1="12" x2="12.01" y1="16" y2="16"/>
            </svg>
        );

        const InterviewAssistant = () => {
            const [isListening, setIsListening] = useState(false);
            const [currentQuestion, setCurrentQuestion] = useState('');
            const [suggestedAnswer, setSuggestedAnswer] = useState('');
            const [isProcessing, setIsProcessing] = useState(false);
            const [error, setError] = useState('');
            const [transcript, setTranscript] = useState([]);
            const recognitionRef = useRef(null);
            const silenceTimerRef = useRef(null);

            // Your professional context
            const cvContext = `
You are helping Emanuil Diamandiev, a Senior Product Manager with the following background:

PROFESSIONAL SUMMARY:
- PMP-certified Product Manager with 8+ years in Business Intelligence, Risk Intelligence, and content-driven products
- Currently at A Data Pro (June 2024-Present) managing Risk Intelligence platforms and AI-powered AML/KYC solutions
- Experience at Bright Marketing Research, Gemseek (Accenture Song), and 15+ years total in project/product management
- PhD Candidate in Project Management (2025-2027), MBA, PMP certified

KEY STRENGTHS:
- Market research & customer insights (NPS, CX analysis)
- Product value propositions & positioning
- Cross-functional team leadership
- Data-informed decision making
- Stakeholder management
- Business process optimization

CURRENT SKILLS:
- Product management: Customer discovery, roadmap planning, Agile/Scrum
- Tools: Jira, Confluence, Power BI, Azure platforms
- Recent experience: 75% product management at A Data Pro, learning API concepts, CI/CD, QA automation
- Areas of growth: IT product management, fintech/payment systems, API platform expertise

CAREER CONTEXT:
- Age 45, looking to grow in product management while leveraging proven project management expertise
- Interested in roles that value customer research, partner insights, and business strategy
- Learning about fintech, payment processing, and developer-focused products
- Prefers roles focusing on business outcomes over deep technical implementation

Based on this context, provide concise, authentic answers that:
1. Highlight relevant experience and skills
2. Show eagerness to learn and grow
3. Connect past achievements to the role requirements
4. Are honest about learning curves while emphasizing strengths
5. Keep answers to 2-3 minutes when spoken
`;

            useEffect(() => {
                // Check for browser support
                if (!('webkitSpeechRecognition' in window) && !('SpeechRecognition' in window)) {
                    setError('Speech recognition not supported in this browser. Please use Chrome or Edge.');
                    return;
                }

                // Initialize speech recognition
                const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
                const recognition = new SpeechRecognition();
                
                recognition.continuous = true;
                recognition.interimResults = true;
                recognition.lang = 'en-US';

                let finalTranscript = '';

                recognition.onresult = (event) => {
                    let interimTranscript = '';
                    
                    for (let i = event.resultIndex; i < event.results.length; i++) {
                        const transcript = event.results[i][0].transcript;
                        if (event.results[i].isFinal) {
                            finalTranscript += transcript + ' ';
                        } else {
                            interimTranscript += transcript;
                        }
                    }

                    setCurrentQuestion(finalTranscript + interimTranscript);

                    // Clear existing silence timer
                    if (silenceTimerRef.current) {
                        clearTimeout(silenceTimerRef.current);
                    }

                    // Set new silence timer - process after 2 seconds of silence
                    if (finalTranscript.trim()) {
                        silenceTimerRef.current = setTimeout(() => {
                            processQuestion(finalTranscript.trim());
                            finalTranscript = '';
                        }, 2000);
                    }
                };

                recognition.onerror = (event) => {
                    console.error('Speech recognition error:', event.error);
                    if (event.error === 'not-allowed') {
                        setError('Microphone access denied. Please allow microphone permissions.');
                        setIsListening(false);
                    }
                };

                recognition.onend = () => {
                    if (isListening) {
                        recognition.start(); // Restart if still in listening mode
                    }
                };

                recognitionRef.current = recognition;

                return () => {
                    if (recognitionRef.current) {
                        recognitionRef.current.stop();
                    }
                    if (silenceTimerRef.current) {
                        clearTimeout(silenceTimerRef.current);
                    }
                };
            }, [isListening]);

            const toggleListening = () => {
                if (!recognitionRef.current) {
                    setError('Speech recognition not initialized');
                    return;
                }

                if (isListening) {
                    recognitionRef.current.stop();
                    setIsListening(false);
                    if (silenceTimerRef.current) {
                        clearTimeout(silenceTimerRef.current);
                    }
                } else {
                    setError('');
                    setCurrentQuestion('');
                    setSuggestedAnswer('');
                    recognitionRef.current.start();
                    setIsListening(true);
                }
            };

            const processQuestion = async (question) => {
                if (!question || isProcessing) return;

                setIsProcessing(true);
                setError('');

                try {
                    const response = await fetch('https://api.anthropic.com/v1/messages', {
                        method: 'POST',
                        headers: {
                            'Content-Type': 'application/json',
                        },
                        body: JSON.stringify({
                            model: 'claude-sonnet-4-20250514',
                            max_tokens: 1500,
                            messages: [
                                {
                                    role: 'user',
                                    content: `${cvContext}

INTERVIEW QUESTION: "${question}"

Provide a strong, authentic answer for this interview question. The answer should:
- Be conversational and natural (2-3 minutes when spoken)
- Draw from Emanuil's real experience and skills
- Show enthusiasm and growth mindset
- Be specific with examples where relevant
- Connect to the role being discussed (likely Product Manager at myPOS or similar)

Format the answer as if Emanuil is speaking directly to the interviewer.`
                                }
                            ]
                        })
                    });

                    if (!response.ok) {
                        throw new Error(`API error: ${response.status}`);
                    }

                    const data = await response.json();
                    const answer = data.content[0].text;
                    
                    setSuggestedAnswer(answer);
                    setTranscript(prev => [...prev, { question, answer, timestamp: new Date() }]);
                    
                    // Speak the answer
                    speakAnswer(answer);
                    
                } catch (err) {
                    console.error('Error processing question:', err);
                    setError('Failed to generate answer. Please try again.');
                } finally {
                    setIsProcessing(false);
                }
            };

            const speakAnswer = (text) => {
                if ('speechSynthesis' in window) {
                    // Cancel any ongoing speech
                    window.speechSynthesis.cancel();
                    
                    const utterance = new SpeechSynthesisUtterance(text);
                    utterance.rate = 0.9;
                    utterance.pitch = 1;
                    utterance.volume = 0.8;
                    
                    window.speechSynthesis.speak(utterance);
                }
            };

            const stopSpeaking = () => {
                if ('speechSynthesis' in window) {
                    window.speechSynthesis.cancel();
                }
            };

            return (
                <div className="min-h-screen bg-gradient-to-br from-blue-50 to-indigo-100 p-6">
                    <div className="max-w-4xl mx-auto">
                        {/* Header */}
                        <div className="bg-white rounded-lg shadow-lg p-6 mb-6">
                            <h1 className="text-3xl font-bold text-gray-800 mb-2">AI Interview Assistant</h1>
                            <p className="text-gray-600">Listen to questions and get AI-powered answers based on your experience</p>
                        </div>

                        {/* Controls */}
                        <div className="bg-white rounded-lg shadow-lg p-6 mb-6">
                            <div className="flex items-center justify-between flex-wrap gap-4">
                                <button
                                    onClick={toggleListening}
                                    className={`flex items-center gap-3 px-6 py-3 rounded-lg font-semibold transition-all ${
                                        isListening
                                            ? 'bg-red-500 hover:bg-red-600 text-white'
                                            : 'bg-blue-500 hover:bg-blue-600 text-white'
                                    }`}
                                >
                                    {isListening ? (
                                        <>
                                            <MicOff size={24} />
                                            Stop Listening
                                        </>
                                    ) : (
                                        <>
                                            <Mic size={24} />
                                            Start Listening
                                        </>
                                    )}
                                </button>

                                <button
                                    onClick={stopSpeaking}
                                    className="flex items-center gap-2 px-4 py-2 bg-gray-500 hover:bg-gray-600 text-white rounded-lg font-semibold transition-all"
                                >
                                    <Volume2 size={20} />
                                    Stop Speaking
                                </button>
                            </div>

                            {isListening && (
                                <div className="mt-4 flex items-center gap-2 text-red-500">
                                    <div className="w-3 h-3 bg-red-500 rounded-full animate-pulse"></div>
                                    <span className="font-medium">Listening...</span>
                                </div>
                            )}
                        </div>

                        {/* Error Display */}
                        {error && (
                            <div className="bg-red-50 border border-red-200 rounded-lg p-4 mb-6 flex items-start gap-3">
                                <AlertCircle className="text-red-500 flex-shrink-0 mt-0.5" size={20} />
                                <p className="text-red-700">{error}</p>
                            </div>
                        )}

                        {/* Current Question */}
                        {currentQuestion && (
                            <div className="bg-white rounded-lg shadow-lg p-6 mb-6">
                                <h2 className="text-lg font-semibold text-gray-800 mb-3 flex items-center gap-2">
                                    <Mic className="text-blue-500" size={20} />
                                    Question Detected:
                                </h2>
                                <p className="text-gray-700 text-lg italic">"{currentQuestion}"</p>
                            </div>
                        )}

                        {/* Processing Indicator */}
                        {isProcessing && (
                            <div className="bg-blue-50 border border-blue-200 rounded-lg p-6 mb-6">
                                <div className="flex items-center gap-3">
                                    <Loader2 className="animate-spin text-blue-500" size={24} />
                                    <p className="text-blue-700 font-medium">Generating your answer...</p>
                                </div>
                            </div>
                        )}

                        {/* Suggested Answer */}
                        {suggestedAnswer && (
                            <div className="bg-white rounded-lg shadow-lg p-6 mb-6">
                                <h2 className="text-lg font-semibold text-gray-800 mb-3 flex items-center gap-2">
                                    <Volume2 className="text-green-500" size={20} />
                                    Suggested Answer:
                                </h2>
                                <div className="prose max-w-none">
                                    <p className="text-gray-700 whitespace-pre-wrap leading-relaxed">
                                        {suggestedAnswer}
                                    </p>
                                </div>
                            </div>
                        )}

                        {/* Interview History */}
                        {transcript.length > 0 && (
                            <div className="bg-white rounded-lg shadow-lg p-6">
                                <h2 className="text-lg font-semibold text-gray-800 mb-4">Interview History</h2>
                                <div className="space-y-4">
                                    {transcript.slice().reverse().map((item, idx) => (
                                        <div key={idx} className="border-l-4 border-blue-500 pl-4 py-2">
                                            <p className="text-sm text-gray-500 mb-1">
                                                {item.timestamp.toLocaleTimeString()}
                                            </p>
                                            <p className="font-medium text-gray-800 mb-2">Q: {item.question}</p>
                                            <p className="text-gray-600 text-sm">A: {item.answer.substring(0, 150)}...</p>
                                        </div>
                                    ))}
                                </div>
                            </div>
                        )}

                        {/* Instructions */}
                        {!isListening && transcript.length === 0 && (
                            <div className="bg-blue-50 border border-blue-200 rounded-lg p-6">
                                <h3 className="font-semibold text-blue-900 mb-3">How to use:</h3>
                                <ol className="list-decimal list-inside space-y-2 text-blue-800">
                                    <li>Click "Start Listening" to activate the microphone</li>
                                    <li>Ask your interview question or let the interviewer ask</li>
                                    <li>Wait 2 seconds after the question ends</li>
                                    <li>The AI will generate and speak a suggested answer</li>
                                    <li>Review the answer and use it as a guide for your response</li>
                                </ol>
                                <p className="mt-4 text-sm text-blue-700">
                                    <strong>Note:</strong> This tool works best in Chrome or Edge browsers. Make sure to allow microphone permissions.
                                </p>
                            </div>
                        )}
                    </div>
                </div>
            );
        };

        // Render the app
        const root = ReactDOM.createRoot(document.getElementById('root'));
        root.render(<InterviewAssistant />);
    </script>
</body>
</html>
